{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(*files):\t\t\t#loading the data from the files \n",
    "\tdata = []\n",
    "\t\n",
    "\tfor file in files:\n",
    "\t\tin_file = open(file, 'rt')\n",
    "\t\ttext = in_file.readlines()\n",
    "\t\tin_file.close()\n",
    "\t\t\n",
    "\t\tfor line in text:\n",
    "\t\t\t#spliting text by whitespaces\n",
    "\t\t\tsentence = line.split()\n",
    "\t\t\t\n",
    "\t\t\t#shrinking vocab size by lowercase\n",
    "\t\t\tsentence = [word.lower() for word in sentence]\n",
    "\t\t\t\n",
    "\t\t\t#using string library to remove punctuations\n",
    "\t\t\t#as used by Jason Brownlee\n",
    "\t\t\timport string \n",
    "\t\t\ttable = str.maketrans('', '', string.punctuation)\n",
    "\t\t\tsentence = [word.translate(table) for word in sentence]\n",
    "\t\t\tdata.append(sentence)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9465f56473cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_naive_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-661227b5b644>\u001b[0m in \u001b[0;36mprep_data\u001b[1;34m(*files)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0min_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0min_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "\tdef __init__(self,files, percentile):\n",
    "\t\tself.data1 = None\n",
    "\t\tself.training_set = None\n",
    "\t\tself.test_set = None\n",
    "\t\tself.document = None\n",
    "\t\tself.class_list = None\n",
    "\t\tself.log_priors = None\n",
    "\t\tself.loglikelihood = None\n",
    "\t\tself.voc_doc = None\n",
    "\t\tself.predictions = None\n",
    "\t\tself.accuracy = None\n",
    "\t\tself.files = files\n",
    "\t\tself.split_percentile = percentile\n",
    "\t\n",
    "\tdef load_data(self):\t\t\t#loading the data from the files \n",
    "\t\tdata = []\n",
    "\t\t\n",
    "\t\tfor file in self.files:\n",
    "\t\t\tin_file = open(file, 'rt')\n",
    "\t\t\ttext = in_file.readlines()\n",
    "\t\t\tin_file.close()\n",
    "\t\t\t\n",
    "\t\t\tfor line in text:\n",
    "\t\t\t\t#spliting text by whitespaces\n",
    "\t\t\t\tsentence = line.split()\n",
    "\t\t\t\t\n",
    "\t\t\t\t#shrinking vocab size by lowercase\n",
    "\t\t\t\tsentence = [word.lower() for word in sentence]\n",
    "\t\t\t\t\n",
    "\t\t\t\t#using string library to remove punctuations\n",
    "\t\t\t\t#as used by Jason Brownlee\n",
    "\t\t\t\timport string \n",
    "\t\t\t\ttable = str.maketrans('', '', string.punctuation)\n",
    "\t\t\t\tsentence = [word.translate(table) for word in sentence]\n",
    "\t\t\t\tdata.append(sentence)\n",
    "\n",
    "\t\tself.data1 = data\n",
    "\t\t\n",
    "\n",
    "\t\n",
    "\tdef spliting_dataset(self):\t\t\t#spliting data into training and test data\n",
    "\t\t\n",
    "\t\tcopy = self.data1\n",
    "\t\ttraining_data_size = int(len(copy) * self.split_percentile)\n",
    "\t\ttraining_set = []\n",
    "\t\t\n",
    "\t\twhile len(training_set) < training_data_size:\n",
    "\t\t\tindex = random.randrange(len(copy))\n",
    "\t\t\ttraining_set.append(copy.pop(index)) \n",
    "\t\t\n",
    "\t\t#after appending to the training set, the remaining items in data will be used for testing\n",
    "\t\t#data being returned is the test data\n",
    "\t\tself.training_set = training_set\n",
    "\t\tself.test_set = copy\n",
    "\t\t\n",
    "\n",
    "\tdef spliting_by_class(self):\n",
    "\t\t\n",
    "\t\tclass_separation = {}\n",
    "\t\tfor i in range(len(self.data1)):\n",
    "\t\t\tsentence = self.data1[i]\n",
    "\t\t\tif sentence[-1] not in class_separation:\n",
    "\t\t\t\tclass_separation[sentence[-1]] = []\n",
    "\t\t\tclass_separation[sentence[-1]].append(sentence[:-1])\n",
    "\t\t\n",
    "\t\tself.document = class_separation\n",
    "\t\tself.class_list = class_separation.keys()\n",
    "\t\t\n",
    "\n",
    "\n",
    "\tdef train_naive_bayes(self):\t\t\t#training naive bayes\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t#training document passed is a dictionary\n",
    "\t\t\n",
    "\t\tlog_priors = []\n",
    "\t\tloglikelihood =  {}\n",
    "\t\tnum_doc = sum([len(self.document[tense]) for tense in self.class_list])\n",
    "\t\t\n",
    "\t\t# Creating vocabulary of all words in document\n",
    "\t\tvoc_doc = set()\n",
    "\t\t\n",
    "\t\tfor class_given in self.class_list:\n",
    "\t\t\tfor tenses in self.document[class_given]:\n",
    "\t\t\t\tfor word in tenses:\n",
    "\t\t\t\t\tvoc_doc.add(word)\n",
    "\n",
    "\t\timport math\n",
    "\t\tfrom collections import Counter\n",
    "\n",
    "\t\tfor class_given in self.class_list:\n",
    "\t\t\tloglikelihood[class_given] = []\n",
    "\t\t\tnum_class = len(self.document[class_given])\n",
    "\t\t\n",
    "\t\t\tlogprior_class_given = math.log(num_class/num_doc)\n",
    "\t\t\t\n",
    "\t\t\tlog_priors.append([class_given, round(logprior_class_given,4)])\n",
    "\t\t\t\n",
    "\t\t\t#creating list of words in given class\n",
    "\t\t\tbigdoc_class = []\n",
    "\t\t\tfor tenses in self.document[class_given]:\n",
    "\t\t\t\tfor word in tenses:\n",
    "\t\t\t\t\tbigdoc_class.append(word)\n",
    "\t\t\t\n",
    "\t\t\twordcount = Counter(bigdoc_class)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\tfor word in voc_doc:\n",
    "\t\t\t\tcount_in_class_given = wordcount[word]\n",
    "\t\t\t\t\n",
    "\t\t\t\tloglikelihood_in_class_given_numerator = count_in_class_given + 1\n",
    "\t\t\t\tloglikelihood_in_class_given_denominator = len(bigdoc_class) + len(voc_doc)\n",
    "\t\t\t\tloglikelihood_in_class_given = math.log(loglikelihood_in_class_given_numerator/ loglikelihood_in_class_given_denominator)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# holding loglikelihoods of each word in each class in a list with index same as class value\n",
    "\t\t\t\tloglikelihood[class_given].append([word, round(loglikelihood_in_class_given,4)])\t\t\t\n",
    "\t\t\t\t\n",
    "\t\tself.log_priors = log_priors\n",
    "\t\tself.loglikelihood = loglikelihood\n",
    "\t\tself.voc_doc = voc_doc\n",
    "\n",
    "\n",
    "\tdef retrieve_loglikelihood(self, loglikelihood_dict,word,class_given):\n",
    "\t\tlikelihood = 0\n",
    "\t\tlist = loglikelihood_dict[class_given]\n",
    "\t\tfor bond in list:\n",
    "\t\t\tif bond[0] == word:\n",
    "\t\t\t\tlikelihood = bond[1]\n",
    "\t\treturn likelihood\n",
    "\n",
    "\n",
    "\tdef retrieve_logprior(self, logprior_list, class_given):\n",
    "\t\tprior = 0\n",
    "\t\tfor list in logprior_list:\n",
    "\t\t\tif list[0] == class_given:\n",
    "\t\t\t\tprior = list[1]\n",
    "\t\treturn prior\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tself.load_data()\n",
    "\t\t\n",
    "\t\tself.spliting_dataset()\n",
    "\t\t\n",
    "\t\tself.spliting_by_class()\n",
    "\t\t\n",
    "\t\tself.train_naive_bayes()\n",
    "\n",
    "\n",
    "\tdef getTestSet(self):\n",
    "\t\treturn self.test_set\n",
    "\n",
    "\tdef getPredictions(self):\n",
    "\t\treturn self.predictions\n",
    "\n",
    "\tdef getAccuracy(self):\n",
    "\t\treturn self.accuracy\n",
    "\n",
    "\tdef test_naive_bayes(self, test_set):\n",
    "\t\targmax_by_class = []\n",
    "\t\tpredictions = []\n",
    "\t\tfor class_given in self.class_list:\n",
    "\t\t\tfor tenses in test_set:\n",
    "\t\t\t\tsum_class_given = self.retrieve_logprior(self.log_priors, class_given)\n",
    "\t\t\t\t#print(tenses)\n",
    "\t\t\t\tfor word in tenses[:-1]:\n",
    "\t\t\t\t\tif word in self.voc_doc:\n",
    "\t\t\t\t\t\tsum_class_given += self.retrieve_loglikelihood(self.loglikelihood,word,class_given)\n",
    "\t\t\t\t\n",
    "\t\t\t\targmax_by_class.append([class_given,sum_class_given])\n",
    "\t\t\n",
    "\t\t#retrieving best predicted class\n",
    "\t\tfor i in range(int((len(argmax_by_class)/2))):\n",
    "\t\t\tif argmax_by_class[i][1] > argmax_by_class[i+int((len(argmax_by_class)/2))][1]:\n",
    "\t\t\t\tpredictions.append(argmax_by_class[i][0])\n",
    "\t\t\telse:\n",
    "\t\t\t\tpredictions.append(argmax_by_class[i+int((len(argmax_by_class)/2))][0])\n",
    "\t\t\n",
    "\t\tself.predictions = predictions\n",
    "\n",
    "\tdef check_accuracy(self, test_set):\n",
    "\t\tcorrect = 0\n",
    "\t\tfor i in range(len(test_set)):\n",
    "\t\t\tif test_set[i][-1] == self.predictions[i]:\n",
    "\t\t\t\tcorrect += 1\n",
    "\t\tself.accuracy = round((correct/float(len(test_set))) * 100.0, 2)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    model = NaiveBayes(['amazon_cells_labelled.txt', 'yelp_labelled.txt', 'imdb_labelled.txt'], 0.9)\n",
    "    model.train()\n",
    "    test_file = sys.argv[1]\n",
    "    test_set = prep_data(test_file)\n",
    "    model.test_naive_bayes(test_set)\n",
    "    predictions = model.getPredictions()\n",
    "\n",
    "    out_file = open('results_file.txt', 'w')\n",
    "    for prediction in predictions:\n",
    "        out_file.write(prediction + \"\\n\")\n",
    "\n",
    "\n",
    "    model.check_accuracy(test_set)\n",
    "    accuracy = model.getAccuracy()\n",
    "    out_file.write(str(accuracy) + \"% accuracy\")\n",
    "\n",
    "    out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
